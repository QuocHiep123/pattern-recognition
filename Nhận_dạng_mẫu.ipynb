{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuocHiep123/pattern-recognition/blob/main/Nh%E1%BA%ADn_d%E1%BA%A1ng_m%E1%BA%ABu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL6qijcVPHl5"
      },
      "outputs": [],
      "source": [
        "# k·∫øt n·ªëi google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLShqB4cPLWs"
      },
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "!pip install underthesea transformers torch accelerate scikit-learn seaborn matplotlib -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import torch\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "# Th∆∞ vi·ªán x·ª≠ l√Ω ng√¥n ng·ªØ & ML c∆° b·∫£n\n",
        "from underthesea import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Th∆∞ vi·ªán Deep Learning (Keras/TensorFlow)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, SpatialDropout1D, SimpleRNN, GRU\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Th∆∞ vi·ªán Transformer (PhoBERT)\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn\n",
        "\n",
        "# Thi·∫øt l·∫≠p device cho PyTorch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y84FIDNJPaOP"
      },
      "outputs": [],
      "source": [
        "# --- 1. T·ª™ ƒêI·ªÇN CHU·∫®N H√ìA ---\n",
        "NORMALIZATION_DICT = {\n",
        "    \"ko\": \"kh√¥ng\", \"k\": \"kh√¥ng\", \"kh\": \"kh√¥ng\", \"khong\": \"kh√¥ng\", \"kg\": \"kh√¥ng\", \"khum\": \"kh√¥ng\",\n",
        "    \"dc\": \"ƒë∆∞·ª£c\", \"ƒëc\": \"ƒë∆∞·ª£c\", \"dk\": \"ƒë∆∞·ª£c\",\n",
        "    \"r\": \"r·ªìi\", \"roi\": \"r·ªìi\",\n",
        "    \"t\": \"t√¥i\", \"mk\": \"m√¨nh\", \"mik\": \"m√¨nh\",\n",
        "    \"sp\": \"s·∫£n ph·∫©m\", \"shop\": \"c·ª≠a h√†ng\",\n",
        "    \"gud\": \"t·ªët\", \"good\": \"t·ªët\", \"oke\": \"t·ªët\", \"ok\": \"t·ªët\", \"oki\": \"t·ªët\",\n",
        "    \"yeu\": \"y√™u\", \"iu\": \"y√™u\",\n",
        "    \"thik\": \"th√≠ch\", \"thic\": \"th√≠ch\",\n",
        "    \"ship\": \"giao h√†ng\",\n",
        "    \"nt\": \"nh·∫Øn tin\", \"ib\": \"nh·∫Øn tin\",\n",
        "    \"tl\": \"tr·∫£ l·ªùi\", \"rep\": \"tr·∫£ l·ªùi\",\n",
        "    \"feed\": \"ƒë√°nh gi√°\", \"feedback\": \"ƒë√°nh gi√°\",\n",
        "    \"ƒë·ªânk\": \"ƒë·ªânh\", \"dink\": \"ƒë·ªânh\",\n",
        "    \"lun\": \"lu√¥n\", \"luN\": \"lu√¥n\", \"lunn\": \"lu√¥n\",\n",
        "    \"x√®g\": \"x√†i\", \"xeg\": \"x√†i\", \"nhu\": \"nh∆∞\",\n",
        "    \"hang\": \"h√†ng\", \"giong\": \"gi·ªëng\", \"hinh\": \"h√¨nh\", \"chat\": \"ch·∫•t\",\n",
        "    \"luong\": \"l∆∞·ª£ng\", \"dep\": \"ƒë·∫πp\", \"xau\": \"x·∫•u\", \"qua\": \"qu√°\",\n",
        "    \"ngon\": \"ngon\", \"thich\": \"th√≠ch\", \"mua\": \"mua\", \"giao\": \"giao\",\n",
        "    \"nhanh\": \"nhanh\", \"la\": \"l√†\", \"rat\": \"r·∫•t\"\n",
        "}\n",
        "\n",
        "EMOJI_DICT = {\n",
        "    \"‚ù§Ô∏è\": \" y√™u_th√≠ch \", \"ü•∞\": \" y√™u_th√≠ch \", \"üòç\": \" y√™u_th√≠ch \", \"üòò\": \" y√™u_th√≠ch \",\n",
        "    \"ü§©\": \" th√≠ch \", \"üëç\": \" t·ªët \", \"üëå\": \" t·ªët \",\n",
        "    \"üòä\": \" vui \", \"üòÅ\": \" vui \", \"üòÇ\": \" c∆∞·ªùi \", \"ü§£\": \" c∆∞·ªùi \",\n",
        "    \"üò°\": \" t·ª©c_gi·∫≠n \", \"ü§¨\": \" ch·ª≠i_b·ªõi \", \"üò§\": \" b·ª±c_m√¨nh \",\n",
        "    \"üò≠\": \" kh√≥c \", \"üò¢\": \" bu·ªìn \", \"‚òπÔ∏è\": \" bu·ªìn \", \"üòû\": \" th·∫•t_v·ªçng \",\n",
        "    \"‚≠ê\": \" sao \", \"üåü\": \" sao \"\n",
        "}\n",
        "\n",
        "!wget https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt -O stopwords.txt\n",
        "def load_safe_stopwords():\n",
        "    with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
        "        stopwords = set([line.strip() for line in f])\n",
        "    whitelist = {\"kh√¥ng\", \"ch·∫≥ng\", \"ch·∫£\", \"ch∆∞a\", \"ƒë·ª´ng\", \"ch·ªõ\", \"ko\", \"k\", \"kh\",\n",
        "                 \"r·∫•t\", \"qu√°\", \"l·∫Øm\", \"v√¥ c√πng\", \"c·ª±c\", \"h∆°i\", \"tuy·ªát\", \"t·ªá\", \"x·∫•u\", \"ƒë·∫πp\", \"ngon\", \"d·ªü\"}\n",
        "    return stopwords - whitelist\n",
        "\n",
        "STOPWORDS_VN = load_safe_stopwords()\n",
        "\n",
        "# --- 2. C√ÅC H√ÄM X·ª¨ L√ù CON ---\n",
        "def normalize_unicode(text):\n",
        "    return unicodedata.normalize('NFC', text)\n",
        "\n",
        "def convert_emoji(text):\n",
        "    for icon, word in EMOJI_DICT.items():\n",
        "        text = text.replace(icon, word)\n",
        "    return text\n",
        "\n",
        "def normalize_teencode(text):\n",
        "    words = text.split()\n",
        "    return \" \".join([NORMALIZATION_DICT.get(word, word) for word in words])\n",
        "\n",
        "def remove_number(text):\n",
        "    return re.sub(r'\\b\\d+\\b', '', text) # Ch·ªâ x√≥a s·ªë ƒë·ª©ng m·ªôt m√¨nh\n",
        "\n",
        "def normalize_currency(text):\n",
        "    text = re.sub(r'(\\d+)\\s*k\\b', r'\\1 ngh√¨n', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'(\\d+)\\s*tr\\b', r'\\1 tri·ªáu', text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "def remove_similarletter(text):\n",
        "    return re.sub(r'([a-zƒë√¢ƒÉ√¥∆°∆∞√™])\\1+', r'\\1', text)\n",
        "\n",
        "def remove_stopword(text):\n",
        "    words = text.split()\n",
        "    return \" \".join([w for w in words if w not in STOPWORDS_VN])\n",
        "\n",
        "def pipeline_processsing(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = normalize_unicode(text)\n",
        "    text = text.lower()\n",
        "    text = convert_emoji(text)\n",
        "    text = normalize_teencode(text)\n",
        "    text = normalize_currency(text)\n",
        "    text = remove_number(text)\n",
        "\n",
        "    # Gi·ªØ l·∫°i ti·∫øng Vi·ªát v√† d·∫•u c√¢u quan tr·ªçng\n",
        "    pattern = r'[a-zaƒÉ√¢√°·∫Ø·∫•√†·∫±·∫ß·∫£·∫≥·∫©√£·∫µ·∫´·∫°·∫∑·∫≠ƒëe√™√©·∫ø√®·ªÅ·∫ª·ªÉ·∫Ω·ªÖ·∫π·ªái√≠√¨·ªâƒ©·ªão√¥∆°√≥·ªë·ªõ√≤·ªì·ªù·ªè·ªï·ªü√µ·ªó·ª°·ªç·ªô·ª£u∆∞√∫·ª©√π·ª´·ªß·ª≠≈©·ªØ·ª•·ª±y√Ω·ª≥·ª∑·ªπ·ªµ!?,_ ]'\n",
        "    char_list = [char if re.match(pattern, char) else ' ' for char in text]\n",
        "    text = ''.join(char_list)\n",
        "\n",
        "    text = remove_similarletter(text)\n",
        "    text = \" \".join(text.split()) # X√≥a kho·∫£ng tr·∫Øng th·ª´a\n",
        "    text = word_tokenize(text, format=\"text\") # T√°ch t·ª´\n",
        "    text = remove_stopword(text)\n",
        "    return text\n",
        "\n",
        "print(\"Test Pipeline:\", pipeline_processsing(\"Sp n√†y gi√° 100k m√† x√†i oke l·∫Øm nha mn ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è!!!\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XXOSupMP4LA"
      },
      "outputs": [],
      "source": [
        "# ƒê·ªçc d·ªØ li·ªáu\n",
        "try:\n",
        "    df = pd.read_csv('/content/drive/MyDrive/dataset_NDM/data.csv')\n",
        "    df = df.rename(columns={'content': 'text', 'label': 'label'})[['text', 'label']].dropna()\n",
        "    print(f\"T·ªïng s·ªë m·∫´u: {len(df)}\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è L·ªói: Kh√¥ng t√¨m th·∫•y file data.csv\")\n",
        "\n",
        "# Chia t·∫≠p Train/Test\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    df['text'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
        ")\n",
        "\n",
        "# L√†m s·∫°ch d·ªØ li·ªáu\n",
        "print(\">>> ƒêang l√†m s·∫°ch d·ªØ li·ªáu (Pre-processing)...\")\n",
        "X_train_clean = X_train_raw.apply(pipeline_processsing)\n",
        "X_test_clean = X_test_raw.apply(pipeline_processsing)\n",
        "print(\"Xong!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3IMe32HQAU0"
      },
      "outputs": [],
      "source": [
        "def build_sentiment_lexicon(X_data, y_data, top_n=2000):\n",
        "    tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
        "    X_tfidf = tfidf.fit_transform(X_data)\n",
        "    vocab = np.array(tfidf.get_feature_names_out())\n",
        "    lexicon = set()\n",
        "    y_arr = np.array(y_data)\n",
        "    for label in np.unique(y_arr):\n",
        "        mask = (y_arr == label)\n",
        "        mean_score = np.asarray(X_tfidf[mask].mean(axis=0)).ravel()\n",
        "        top_indices = mean_score.argsort()[-top_n:][::-1]\n",
        "        lexicon.update(vocab[top_indices])\n",
        "    return list(lexicon)\n",
        "\n",
        "my_lexicon = build_sentiment_lexicon(X_train_clean, y_train)\n",
        "\n",
        "ml_models = [\n",
        "    ('Na√Øve Bayes (Lexicon)', Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(ngram_range=(1,2), vocabulary=my_lexicon)),\n",
        "        ('nb', MultinomialNB())\n",
        "    ])),\n",
        "    ('Logistic Regression', Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(ngram_range=(1,2), max_features=15000)),\n",
        "        ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
        "    ])),\n",
        "    ('Linear SVM', Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(ngram_range=(1,2), max_features=15000)),\n",
        "        ('clf', LinearSVC(class_weight='balanced', dual='auto'))\n",
        "    ])),\n",
        "    ('Random Forest', Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(ngram_range=(1,2), max_features=15000)),\n",
        "        ('clf', RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=-1))\n",
        "    ]))\n",
        "]\n",
        "\n",
        "# 3. Hu·∫•n luy·ªán\n",
        "results = []\n",
        "print(\"\\n>>> K·∫æT QU·∫¢ MACHINE LEARNING:\")\n",
        "for name, model in ml_models:\n",
        "    model.fit(X_train_clean, y_train)\n",
        "    y_pred = model.predict(X_test_clean)\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    neu_score = report['NEU']['f1-score'] if 'NEU' in report else 0\n",
        "    print(f\"- {name}: Acc={report['accuracy']:.2%}, Macro F1={report['macro avg']['f1-score']:.2%}\")\n",
        "    results.append({\n",
        "        'Model': name, 'Accuracy': report['accuracy'],\n",
        "        'Macro F1': report['macro avg']['f1-score'], 'NEU F1': neu_score\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuFsur83QErA"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 15000\n",
        "MAX_LEN = 100\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train_clean)\n",
        "\n",
        "# Padding\n",
        "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train_clean), maxlen=MAX_LEN, padding='post')\n",
        "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test_clean), maxlen=MAX_LEN, padding='post')\n",
        "\n",
        "# Encode Labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_enc = label_encoder.fit_transform(y_train)\n",
        "y_test_enc = label_encoder.transform(y_test)\n",
        "y_train_cat = to_categorical(y_train_enc)\n",
        "y_test_cat = to_categorical(y_test_enc)\n",
        "\n",
        "# L·∫•y index c·ªßa l·ªõp NEU ƒë·ªÉ theo d√µi\n",
        "neu_idx = str(label_encoder.transform(['NEU'])[0]) if 'NEU' in label_encoder.classes_ else '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lvYf3cFW9MmA"
      },
      "outputs": [],
      "source": [
        "def build_dl_model(model_type):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LEN))\n",
        "\n",
        "    if model_type == 'SimpleRNN':\n",
        "        model.add(SimpleRNN(64))\n",
        "    elif model_type == 'GRU':\n",
        "        model.add(GRU(64))\n",
        "    elif model_type == 'LSTM':\n",
        "        model.add(LSTM(64))\n",
        "    elif model_type == 'Bi-LSTM':\n",
        "        model.add(Bidirectional(LSTM(64)))\n",
        "\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "dl_model_names = ['SimpleRNN', 'GRU', 'LSTM', 'Bi-LSTM']\n",
        "\n",
        "print(\"\\n>>> K·∫æT QU·∫¢ DEEP LEARNING:\")\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "for m_name in dl_model_names:\n",
        "    print(f\"‚öôÔ∏è Training {m_name}...\")\n",
        "    model = build_dl_model(m_name)\n",
        "    model.fit(X_train_seq, y_train_cat, epochs=10, batch_size=64, validation_split=0.1, callbacks=[early_stop], verbose=0)\n",
        "\n",
        "    y_pred_probs = model.predict(X_test_seq)\n",
        "    y_pred_idx = np.argmax(y_pred_probs, axis=1)\n",
        "    report = classification_report(y_test_enc, y_pred_idx, output_dict=True)\n",
        "\n",
        "    neu_score = report[neu_idx]['f1-score'] if neu_idx in report else 0\n",
        "    print(f\" -> Acc={report['accuracy']:.2%}, Macro F1={report['macro avg']['f1-score']:.2%}\")\n",
        "\n",
        "    results.append({\n",
        "        'Model': m_name, 'Accuracy': report['accuracy'],\n",
        "        'Macro F1': report['macro avg']['f1-score'], 'NEU F1': neu_score\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyAMr0AcQFw6",
        "outputId": "5a50b5f2-85be-483c-d2c5-a125b0341106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1136' max='4716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1136/4716 05:04 < 16:02, 3.72 it/s, Epoch 0.72/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_enc), y=y_train_enc)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "print(f\"Class Weights t·ª± ƒë·ªông t√≠nh to√°n: {class_weights}\")\n",
        "\n",
        "MODEL_NAME = \"vinai/phobert-base\"\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
        "model_bert = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3).to(device)\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text, add_special_tokens=True, max_length=self.max_len,\n",
        "            padding='max_length', truncation=True, return_attention_mask=True, return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = SentimentDataset(X_train_clean.to_list(), y_train_enc, tokenizer_bert, MAX_LEN)\n",
        "test_dataset = SentimentDataset(X_test_clean.to_list(), y_test_enc, tokenizer_bert, MAX_LEN)\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        # √Åp d·ª•ng tr·ªçng s·ªë ƒë√£ t√≠nh ·ªü tr√™n\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    macro_f1 = f1_score(labels, preds, average='macro')\n",
        "    return {'accuracy': acc, 'macro_f1': macro_f1}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results', num_train_epochs=3, # Ch·∫°y 3 v√≤ng l√† ƒë·ªß\n",
        "    per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
        "    warmup_steps=500, weight_decay=0.01, logging_dir='./logs',\n",
        "    eval_strategy=\"epoch\", save_strategy=\"no\", learning_rate=2e-5, report_to=\"none\"\n",
        ")\n",
        "\n",
        "# 4. Hu·∫•n luy·ªán\n",
        "print(\"\\n>>> FINE-TUNING PHOBERT (C√ì TR·ªåNG S·ªê)...\")\n",
        "trainer = CustomTrainer(\n",
        "    model=model_bert, args=training_args,\n",
        "    train_dataset=train_dataset, eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# 5. ƒê√°nh gi√°\n",
        "print(\"\\n>>> ƒê√ÅNH GI√Å K·∫æT QU·∫¢ PHOBERT...\")\n",
        "preds_output = trainer.predict(test_dataset)\n",
        "y_pred_bert = preds_output.predictions.argmax(-1)\n",
        "report_bert = classification_report(y_test_enc, y_pred_bert, output_dict=True)\n",
        "\n",
        "results.append({\n",
        "    'Model': 'PhoBERT (Weighted)', 'Accuracy': report_bert['accuracy'],\n",
        "    'Macro F1': report_bert['macro avg']['f1-score'],\n",
        "    'NEU F1': report_bert[str(label_encoder.transform(['NEU'])[0])]['f1-score']\n",
        "})\n",
        "print(f\"PhoBERT Accuracy: {report_bert['accuracy']:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUK3xmkaQHyI"
      },
      "outputs": [],
      "source": [
        "df_res = pd.DataFrame(results).sort_values(by='Macro F1', ascending=False)\n",
        "pd.options.display.float_format = '{:.2%}'.format\n",
        "print(\"\\nüèÜ B·∫¢NG X·∫æP H·∫†NG CU·ªêI C√ôNG:\")\n",
        "print(df_res)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=df_res, x='Macro F1', y='Model', palette='viridis')\n",
        "plt.title('So s√°nh ƒë·ªô hi·ªáu qu·∫£ (Macro F1) gi·ªØa c√°c m√¥ h√¨nh')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVzLccvi6qbF"
      },
      "source": [
        "T√†i li·ªáu tham kh·∫£o :\n",
        "\n",
        "Thu·∫≠t to√°n naive Bayes: https://www.ijml.org/papers/409-LC010.pdf\n",
        "\n",
        "TF-IDF: https://aclanthology.org/P10-1141.pdf\n",
        "\n",
        "Logistic Regression: https://sci-hub.mk/10.1111/j.2517-6161.1958.tb00292.x\n",
        "\n",
        "Support Vector Machine (SVM): https://homepages.math.uic.edu/~lreyzin/papers/cortes95.pdf\n",
        "\n",
        "Random Forest: https://jmlr.org/papers/volume13/biau12a/biau12a.pdf\n",
        "\n",
        "LSTM: https://www.researchgate.net/publication/13853244_Long_Short-Term_Memory\n",
        "\n",
        "GRU (Gated Recurrent Unit):https://arxiv.org/pdf/1406.1078\n",
        "\n",
        "Bi-LSTM (Bidirectional LSTM): https://www.sciencedirect.com/topics/computer-science/bidirectional-long-short-term-memory-network\n",
        "\n",
        "PhoBERT: Pre-trained language models for Vietnamese\n",
        "https://aclanthology.org/2020.findings-emnlp.92.pdf\n",
        "\n",
        "\n",
        "VNCoreNLP: https://aclanthology.org/N18-5012.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yD-0JA-_SWU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyO4Nab23SQtND6RxRSFvlBc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}